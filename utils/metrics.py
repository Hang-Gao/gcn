"""
è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ¨¡å—
"""

import numpy as np
from typing import List, Dict, Any, Tuple
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy import stats
import logging

logger = logging.getLogger(__name__)


class MetricsCalculator:
    """
    æŒ‡æ ‡è®¡ç®—å™¨ç±»
    """
    
    @staticmethod
    def calculate_basic_metrics(predictions: List[float], targets: List[float]) -> Dict[str, float]:
        """
        è®¡ç®—åŸºç¡€å›å½’æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹å€¼åˆ—è¡¨
            targets: çœŸå®å€¼åˆ—è¡¨
            
        Returns:
            åŸºç¡€æŒ‡æ ‡å­—å…¸
        """
        predictions = np.array(predictions)
        targets = np.array(targets)
        
        mse = mean_squared_error(targets, predictions)
        mae = mean_absolute_error(targets, predictions)
        r2 = r2_score(targets, predictions)
        rmse = np.sqrt(mse)
        
        # ç›¸å…³ç³»æ•°
        correlation = np.corrcoef(predictions, targets)[0, 1] if len(predictions) > 1 else 0.0
        
        return {
            'mse': mse,
            'mae': mae,
            'r2': r2,
            'rmse': rmse,
            'correlation': correlation
        }
    
    @staticmethod
    def calculate_error_statistics(predictions: List[float], targets: List[float]) -> Dict[str, Any]:
        """
        è®¡ç®—è¯¯å·®ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            predictions: é¢„æµ‹å€¼åˆ—è¡¨
            targets: çœŸå®å€¼åˆ—è¡¨
            
        Returns:
            è¯¯å·®ç»Ÿè®¡å­—å…¸
        """
        predictions = np.array(predictions)
        targets = np.array(targets)
        errors = predictions - targets
        abs_errors = np.abs(errors)
        
        # åŸºç¡€ç»Ÿè®¡
        error_stats = {
            'mean_error': np.mean(errors),
            'std_error': np.std(errors),
            'max_error': np.max(abs_errors),
            'min_error': np.min(abs_errors),
            'median_error': np.median(errors),
            'median_abs_error': np.median(abs_errors)
        }
        
        # åˆ†ä½æ•°
        percentiles = [10, 25, 50, 75, 90, 95, 99]
        error_stats['error_percentiles'] = {}
        for p in percentiles:
            error_stats['error_percentiles'][f'{p}%'] = np.percentile(abs_errors, p)
        
        # ç›¸å¯¹è¯¯å·®ï¼ˆé¿å…é™¤é›¶ï¼‰
        relative_errors = abs_errors / (np.abs(targets) + 1e-8)
        error_stats['mean_relative_error'] = np.mean(relative_errors)
        error_stats['median_relative_error'] = np.median(relative_errors)
        
        return error_stats
    
    @staticmethod
    def calculate_distribution_metrics(predictions: List[float], targets: List[float]) -> Dict[str, float]:
        """
        è®¡ç®—åˆ†å¸ƒç›¸å…³æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹å€¼åˆ—è¡¨
            targets: çœŸå®å€¼åˆ—è¡¨
            
        Returns:
            åˆ†å¸ƒæŒ‡æ ‡å­—å…¸
        """
        predictions = np.array(predictions)
        targets = np.array(targets)
        errors = predictions - targets
        
        # æ­£æ€æ€§æ£€éªŒ (Shapiro-Wilk test)
        if len(errors) >= 3:
            shapiro_stat, shapiro_p = stats.shapiro(errors)
        else:
            shapiro_stat, shapiro_p = 0.0, 1.0
        
        # ååº¦å’Œå³°åº¦
        skewness = stats.skew(errors)
        kurtosis = stats.kurtosis(errors)
        
        # Kolmogorov-Smirnov æ­£æ€æ€§æ£€éªŒ
        if len(errors) >= 3:
            ks_stat, ks_p = stats.kstest(errors, 'norm')
        else:
            ks_stat, ks_p = 0.0, 1.0
        
        return {
            'error_skewness': skewness,
            'error_kurtosis': kurtosis,
            'shapiro_stat': shapiro_stat,
            'shapiro_pvalue': shapiro_p,
            'ks_stat': ks_stat,
            'ks_pvalue': ks_p,
            'is_normal_shapiro': shapiro_p > 0.05,
            'is_normal_ks': ks_p > 0.05
        }
    
    @staticmethod
    def calculate_prediction_quality(predictions: List[float], targets: List[float]) -> Dict[str, Any]:
        """
        è®¡ç®—é¢„æµ‹è´¨é‡æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹å€¼åˆ—è¡¨
            targets: çœŸå®å€¼åˆ—è¡¨
            
        Returns:
            é¢„æµ‹è´¨é‡æŒ‡æ ‡å­—å…¸
        """
        predictions = np.array(predictions)
        targets = np.array(targets)
        
        # é¢„æµ‹å‡†ç¡®æ€§åˆ†çº§
        abs_errors = np.abs(predictions - targets)
        target_range = np.max(targets) - np.min(targets)
        
        # å®šä¹‰å‡†ç¡®æ€§é˜ˆå€¼ï¼ˆåŸºäºç›®æ ‡å€¼èŒƒå›´çš„ç™¾åˆ†æ¯”ï¼‰
        thresholds = {
            'excellent': 0.01 * target_range,  # 1%
            'good': 0.03 * target_range,       # 3%
            'fair': 0.05 * target_range,       # 5%
            'poor': 0.10 * target_range        # 10%
        }
        
        # è®¡ç®—å„å‡†ç¡®æ€§çº§åˆ«çš„æ ·æœ¬æ¯”ä¾‹
        quality_counts = {
            'excellent': np.sum(abs_errors <= thresholds['excellent']),
            'good': np.sum((abs_errors > thresholds['excellent']) & (abs_errors <= thresholds['good'])),
            'fair': np.sum((abs_errors > thresholds['good']) & (abs_errors <= thresholds['fair'])),
            'poor': np.sum((abs_errors > thresholds['fair']) & (abs_errors <= thresholds['poor'])),
            'very_poor': np.sum(abs_errors > thresholds['poor'])
        }
        
        total_samples = len(predictions)
        quality_ratios = {k: v / total_samples for k, v in quality_counts.items()}
        
        return {
            'quality_counts': quality_counts,
            'quality_ratios': quality_ratios,
            'thresholds': thresholds,
            'target_range': target_range,
            'total_samples': total_samples
        }


def calculate_comprehensive_metrics(predictions: List[float], 
                                  targets: List[float],
                                  detailed: bool = True) -> Dict[str, Any]:
    """
    è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡
    
    Args:
        predictions: é¢„æµ‹å€¼åˆ—è¡¨
        targets: çœŸå®å€¼åˆ—è¡¨
        detailed: æ˜¯å¦è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        
    Returns:
        ç»¼åˆæŒ‡æ ‡å­—å…¸
    """
    calculator = MetricsCalculator()
    
    # åŸºç¡€æŒ‡æ ‡
    basic_metrics = calculator.calculate_basic_metrics(predictions, targets)
    result = {'basic_metrics': basic_metrics}
    
    if detailed:
        # è¯¯å·®ç»Ÿè®¡
        error_stats = calculator.calculate_error_statistics(predictions, targets)
        result['error_statistics'] = error_stats
        
        # åˆ†å¸ƒæŒ‡æ ‡
        distribution_metrics = calculator.calculate_distribution_metrics(predictions, targets)
        result['distribution_metrics'] = distribution_metrics
        
        # é¢„æµ‹è´¨é‡
        quality_metrics = calculator.calculate_prediction_quality(predictions, targets)
        result['quality_metrics'] = quality_metrics
    
    # æ·»åŠ å…ƒä¿¡æ¯
    result['meta'] = {
        'num_samples': len(predictions),
        'target_range': np.max(targets) - np.min(targets),
        'target_mean': np.mean(targets),
        'target_std': np.std(targets),
        'prediction_mean': np.mean(predictions),
        'prediction_std': np.std(predictions)
    }
    
    logger.info(f"æŒ‡æ ‡è®¡ç®—å®Œæˆï¼Œæ ·æœ¬æ•°: {len(predictions)}")
    logger.info(f"åŸºç¡€æŒ‡æ ‡ - MSE: {basic_metrics['mse']:.6f}, RÂ²: {basic_metrics['r2']:.4f}")
    
    return result


def print_metrics_summary(metrics: Dict[str, Any], title: str = "æ¨¡å‹è¯„ä¼°ç»“æœ") -> None:
    """
    æ‰“å°æŒ‡æ ‡æ‘˜è¦
    
    Args:
        metrics: æŒ‡æ ‡å­—å…¸
        title: æ ‡é¢˜
    """
    print(f"\n{'='*50}")
    print(f"{title:^50}")
    print(f"{'='*50}")
    
    # åŸºç¡€æŒ‡æ ‡
    if 'basic_metrics' in metrics:
        basic = metrics['basic_metrics']
        print(f"\nğŸ“Š åŸºç¡€æŒ‡æ ‡:")
        print(f"  MSE (å‡æ–¹è¯¯å·®):        {basic['mse']:.6f}")
        print(f"  MAE (å¹³å‡ç»å¯¹è¯¯å·®):     {basic['mae']:.6f}")
        print(f"  RMSE (å‡æ–¹æ ¹è¯¯å·®):      {basic['rmse']:.6f}")
        print(f"  RÂ² (å†³å®šç³»æ•°):         {basic['r2']:.4f}")
        print(f"  ç›¸å…³ç³»æ•°:              {basic['correlation']:.4f}")
    
    # å…ƒä¿¡æ¯
    if 'meta' in metrics:
        meta = metrics['meta']
        print(f"\nğŸ“ˆ æ•°æ®æ¦‚è§ˆ:")
        print(f"  æ ·æœ¬æ•°é‡:              {meta['num_samples']}")
        print(f"  ç›®æ ‡å€¼èŒƒå›´:            {meta['target_range']:.6f}")
        print(f"  ç›®æ ‡å€¼å‡å€¼:            {meta['target_mean']:.6f}")
        print(f"  ç›®æ ‡å€¼æ ‡å‡†å·®:          {meta['target_std']:.6f}")
    
    # è¯¯å·®ç»Ÿè®¡
    if 'error_statistics' in metrics:
        error = metrics['error_statistics']
        print(f"\nğŸ¯ è¯¯å·®ç»Ÿè®¡:")
        print(f"  å¹³å‡è¯¯å·®:              {error['mean_error']:.6f}")
        print(f"  è¯¯å·®æ ‡å‡†å·®:            {error['std_error']:.6f}")
        print(f"  æœ€å¤§ç»å¯¹è¯¯å·®:          {error['max_error']:.6f}")
        print(f"  ä¸­ä½æ•°ç»å¯¹è¯¯å·®:        {error['median_abs_error']:.6f}")
        print(f"  å¹³å‡ç›¸å¯¹è¯¯å·®:          {error['mean_relative_error']:.4f}")
    
    # é¢„æµ‹è´¨é‡
    if 'quality_metrics' in metrics:
        quality = metrics['quality_metrics']
        ratios = quality['quality_ratios']
        print(f"\nâ­ é¢„æµ‹è´¨é‡åˆ†å¸ƒ:")
        print(f"  ä¼˜ç§€ (è¯¯å·®<1%):        {ratios['excellent']:.1%}")
        print(f"  è‰¯å¥½ (è¯¯å·®1-3%):       {ratios['good']:.1%}")
        print(f"  ä¸€èˆ¬ (è¯¯å·®3-5%):       {ratios['fair']:.1%}")
        print(f"  è¾ƒå·® (è¯¯å·®5-10%):      {ratios['poor']:.1%}")
        print(f"  å¾ˆå·® (è¯¯å·®>10%):       {ratios['very_poor']:.1%}")
    
    # åˆ†å¸ƒç‰¹æ€§
    if 'distribution_metrics' in metrics:
        dist = metrics['distribution_metrics']
        print(f"\nğŸ“Š è¯¯å·®åˆ†å¸ƒç‰¹æ€§:")
        print(f"  ååº¦:                 {dist['error_skewness']:.4f}")
        print(f"  å³°åº¦:                 {dist['error_kurtosis']:.4f}")
        print(f"  æ­£æ€æ€§(Shapiro):       {'âœ“' if dist['is_normal_shapiro'] else 'âœ—'} (p={dist['shapiro_pvalue']:.4f})")
        print(f"  æ­£æ€æ€§(K-S):          {'âœ“' if dist['is_normal_ks'] else 'âœ—'} (p={dist['ks_pvalue']:.4f})")
    
    print(f"\n{'='*50}")


def compare_models(model_results: Dict[str, Dict[str, Any]], 
                  metric_name: str = 'r2') -> None:
    """
    æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½
    
    Args:
        model_results: æ¨¡å‹ç»“æœå­—å…¸ï¼Œæ ¼å¼ä¸º {model_name: metrics_dict}
        metric_name: ç”¨äºæ¯”è¾ƒçš„ä¸»è¦æŒ‡æ ‡åç§°
    """
    print(f"\n{'='*60}")
    print(f"{'æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ':^60}")
    print(f"{'='*60}")
    
    # æå–æ¯”è¾ƒæŒ‡æ ‡
    comparison_data = []
    for model_name, metrics in model_results.items():
        if 'basic_metrics' in metrics:
            basic = metrics['basic_metrics']
            comparison_data.append({
                'model': model_name,
                'mse': basic['mse'],
                'mae': basic['mae'], 
                'r2': basic['r2'],
                'correlation': basic['correlation']
            })
    
    if not comparison_data:
        print("æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹ç»“æœç”¨äºæ¯”è¾ƒ")
        return
    
    # æŒ‰æŒ‡å®šæŒ‡æ ‡æ’åºï¼ˆRÂ²è¶Šå¤§è¶Šå¥½ï¼Œå…¶ä»–æŒ‡æ ‡è¶Šå°è¶Šå¥½ï¼‰
    reverse_sort = metric_name in ['r2', 'correlation']
    comparison_data.sort(key=lambda x: x[metric_name], reverse=reverse_sort)
    
    # æ‰“å°æ¯”è¾ƒè¡¨æ ¼
    print(f"{'æ’å':<4} {'æ¨¡å‹åç§°':<20} {'MSE':<12} {'MAE':<12} {'RÂ²':<8} {'ç›¸å…³æ€§':<8}")
    print("-" * 60)
    
    for i, data in enumerate(comparison_data, 1):
        print(f"{i:<4} {data['model']:<20} {data['mse']:<12.6f} {data['mae']:<12.6f} "
              f"{data['r2']:<8.4f} {data['correlation']:<8.4f}")
    
    # æœ€ä½³æ¨¡å‹
    best_model = comparison_data[0]
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['model']}")
    print(f"   {metric_name.upper()}: {best_model[metric_name]:.6f}")
    
    print(f"\n{'='*60}")


def generate_metrics_report(metrics: Dict[str, Any], 
                          save_path: str = None) -> str:
    """
    ç”ŸæˆæŒ‡æ ‡æŠ¥å‘Š
    
    Args:
        metrics: æŒ‡æ ‡å­—å…¸
        save_path: æŠ¥å‘Šä¿å­˜è·¯å¾„
        
    Returns:
        æŠ¥å‘Šæ–‡æœ¬
    """
    from datetime import datetime
    
    report_lines = []
    report_lines.append("# Alpha GCN æ¨¡å‹è¯„ä¼°æŠ¥å‘Š")
    report_lines.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("\n" + "="*50)
    
    # åŸºç¡€æŒ‡æ ‡
    if 'basic_metrics' in metrics:
        basic = metrics['basic_metrics']
        report_lines.append("\n## åŸºç¡€æ€§èƒ½æŒ‡æ ‡")
        report_lines.append(f"- **MSE (å‡æ–¹è¯¯å·®)**: {basic['mse']:.6f}")
        report_lines.append(f"- **MAE (å¹³å‡ç»å¯¹è¯¯å·®)**: {basic['mae']:.6f}")
        report_lines.append(f"- **RMSE (å‡æ–¹æ ¹è¯¯å·®)**: {basic['rmse']:.6f}")
        report_lines.append(f"- **RÂ² (å†³å®šç³»æ•°)**: {basic['r2']:.4f}")
        report_lines.append(f"- **Pearsonç›¸å…³ç³»æ•°**: {basic['correlation']:.4f}")
    
    # æ•°æ®æ¦‚è§ˆ
    if 'meta' in metrics:
        meta = metrics['meta']
        report_lines.append("\n## æ•°æ®æ¦‚è§ˆ")
        report_lines.append(f"- **æµ‹è¯•æ ·æœ¬æ•°**: {meta['num_samples']}")
        report_lines.append(f"- **ç›®æ ‡å€¼èŒƒå›´**: {meta['target_range']:.6f}")
        report_lines.append(f"- **ç›®æ ‡å€¼å‡å€¼**: {meta['target_mean']:.6f} Â± {meta['target_std']:.6f}")
        report_lines.append(f"- **é¢„æµ‹å€¼å‡å€¼**: {meta['prediction_mean']:.6f} Â± {meta['prediction_std']:.6f}")
    
    # è¯¯å·®åˆ†æ
    if 'error_statistics' in metrics:
        error = metrics['error_statistics']
        report_lines.append("\n## è¯¯å·®åˆ†æ")
        report_lines.append(f"- **å¹³å‡è¯¯å·®**: {error['mean_error']:.6f}")
        report_lines.append(f"- **è¯¯å·®æ ‡å‡†å·®**: {error['std_error']:.6f}")
        report_lines.append(f"- **æœ€å¤§ç»å¯¹è¯¯å·®**: {error['max_error']:.6f}")
        report_lines.append(f"- **å¹³å‡ç›¸å¯¹è¯¯å·®**: {error['mean_relative_error']:.4f}")
        
        report_lines.append("\n### è¯¯å·®åˆ†ä½æ•°")
        for percentile, value in error['error_percentiles'].items():
            report_lines.append(f"- **{percentile}åˆ†ä½æ•°**: {value:.6f}")
    
    # é¢„æµ‹è´¨é‡
    if 'quality_metrics' in metrics:
        quality = metrics['quality_metrics']
        ratios = quality['quality_ratios']
        report_lines.append("\n## é¢„æµ‹è´¨é‡åˆ†å¸ƒ")
        report_lines.append(f"- **ä¼˜ç§€é¢„æµ‹** (è¯¯å·®<1%): {ratios['excellent']:.1%}")
        report_lines.append(f"- **è‰¯å¥½é¢„æµ‹** (è¯¯å·®1-3%): {ratios['good']:.1%}")
        report_lines.append(f"- **ä¸€èˆ¬é¢„æµ‹** (è¯¯å·®3-5%): {ratios['fair']:.1%}")
        report_lines.append(f"- **è¾ƒå·®é¢„æµ‹** (è¯¯å·®5-10%): {ratios['poor']:.1%}")
        report_lines.append(f"- **å¾ˆå·®é¢„æµ‹** (è¯¯å·®>10%): {ratios['very_poor']:.1%}")
    
    # åˆ†å¸ƒç‰¹æ€§
    if 'distribution_metrics' in metrics:
        dist = metrics['distribution_metrics']
        report_lines.append("\n## è¯¯å·®åˆ†å¸ƒç‰¹æ€§")
        report_lines.append(f"- **ååº¦**: {dist['error_skewness']:.4f}")
        report_lines.append(f"- **å³°åº¦**: {dist['error_kurtosis']:.4f}")
        report_lines.append(f"- **æ­£æ€æ€§æ£€éªŒ(Shapiro-Wilk)**: p={dist['shapiro_pvalue']:.4f} ({'é€šè¿‡' if dist['is_normal_shapiro'] else 'ä¸é€šè¿‡'})")
        report_lines.append(f"- **æ­£æ€æ€§æ£€éªŒ(Kolmogorov-Smirnov)**: p={dist['ks_pvalue']:.4f} ({'é€šè¿‡' if dist['is_normal_ks'] else 'ä¸é€šè¿‡'})")
    
    report_text = "\n".join(report_lines)
    
    if save_path:
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
    
    return report_text